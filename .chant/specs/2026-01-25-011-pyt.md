---
type: code
status: completed
target_files:
- src/agent.rs
model: claude-haiku-4-5-20251001
---

# Fix ollama streaming output - tokens displayed one per line instead of continuous text

## Problem

When using ollama provider, streaming output displays each token on its own line:
```
 use
 std
::
path
::
Path
;
```

Instead of continuous text:
```
use std::path::Path;
```

## Root Cause

In `src/provider.rs` OllamaProvider::invoke (lines 179-206):

1. Ollama sends small token chunks via SSE (e.g., `"use"`, `" "`, `"std"`)
2. Code calls `content.lines()` on each tiny chunk
3. `callback(text_line)` is called for each "line" (which is just one token)
4. The callback in cmd/agent.rs prints each call, resulting in one token per line

## Fix

Buffer content and only call callback when we have a complete line (ends with `\n`), or flush remaining content at the end.

```rust
// Add a line buffer before the loop
let mut line_buffer = String::new();

// Inside the content handling:
line_buffer.push_str(content);

// Only callback when we have complete lines
while let Some(newline_pos) = line_buffer.find('\n') {
    let line = &line_buffer[..newline_pos];
    callback(line)?;
    captured_output.push_str(line);
    captured_output.push('\n');
    line_buffer = line_buffer[newline_pos + 1..].to_string();
}

// After the loop, flush any remaining content
if !line_buffer.is_empty() {
    callback(&line_buffer)?;
    captured_output.push_str(&line_buffer);
}
```

## Acceptance Criteria

- [x] Ollama streaming output displays as continuous text, not one token per line
- [x] Lines are printed when a newline is encountered in the stream
- [x] Remaining buffered content is flushed at end of stream
- [x] OpenAI provider updated with same fix (has identical code pattern)
- [x] `just check` passes (library tests)
- [x] Manual test: `just chant work <spec-id>` with ollama shows readable output
