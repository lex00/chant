---
type: code
status: pending
target_files:
- Cargo.toml
- src/agent.rs
- src/tools.rs
- src/provider.rs
---

# Add ollama-rs agent runtime with tool calling

## Problem

Currently, when using `provider: ollama` in chant config, the model receives a prompt but has no way to execute tools (read files, write files, run commands). The model outputs plans but cannot actually implement them.

Claude CLI has a built-in agent runtime that handles:
1. Model thinks and requests tool call
2. Runtime executes tool
3. Runtime feeds result back to model
4. Loop continues until task complete

Chant needs this same capability for local LLM execution.

## Solution

Add [ollama-rs](https://github.com/pepperoni21/ollama-rs) with its `Coordinator` feature that provides:
- Built-in agent loop (think → tool → execute → feedback)
- `#[function]` macro to define tools as regular Rust functions
- Structured output support

## Implementation

### 1. Add dependency

```toml
# Cargo.toml
[dependencies]
ollama-rs = { version = "0.3", features = ["function-calling"] }
```

### 2. Create src/tools.rs with chant tools

```rust
use ollama_rs_macros::function;
use std::fs;
use std::process::Command;

/// Read the contents of a file at the given path.
/// Use this to understand existing code before making changes.
#[function]
pub fn read_file(path: String) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
    Ok(fs::read_to_string(&path)?)
}

/// Write content to a file at the given path.
/// Creates the file if it doesn't exist, overwrites if it does.
#[function]
pub fn write_file(path: String, content: String) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
    fs::write(&path, &content)?;
    Ok(format!("Wrote {} bytes to {}", content.len(), path))
}

/// Run a shell command and return its output.
/// Use for: git operations, cargo build/test, file operations.
#[function]
pub fn run_command(command: String) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
    let output = Command::new("sh")
        .arg("-c")
        .arg(&command)
        .output()?;

    let stdout = String::from_utf8_lossy(&output.stdout);
    let stderr = String::from_utf8_lossy(&output.stderr);

    if output.status.success() {
        Ok(stdout.to_string())
    } else {
        Ok(format!("Command failed:\nstdout: {}\nstderr: {}", stdout, stderr))
    }
}

/// List files matching a glob pattern.
/// Example: list_files("src/**/*.rs") returns all Rust files in src/
#[function]
pub fn list_files(pattern: String) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
    let paths: Vec<_> = glob::glob(&pattern)?
        .filter_map(Result::ok)
        .map(|p| p.display().to_string())
        .collect();
    Ok(paths.join("\n"))
}
```

### 3. Create src/agent.rs with Coordinator integration

```rust
use ollama_rs::Ollama;
use ollama_rs::coordinator::Coordinator;
use ollama_rs::models::chat::ChatMessage;
use anyhow::Result;

use crate::tools::{ReadFile, WriteFile, RunCommand, ListFiles};

pub async fn run_agent(
    endpoint: &str,
    model: &str,
    system_prompt: &str,
    user_message: &str,
    callback: &mut dyn FnMut(&str) -> Result<()>,
) -> Result<String> {
    let ollama = Ollama::from_url(endpoint.to_string());

    let tools = vec![
        Box::new(ReadFile) as Box<dyn ollama_rs::coordinator::Tool>,
        Box::new(WriteFile),
        Box::new(RunCommand),
        Box::new(ListFiles),
    ];

    let mut coordinator = Coordinator::new(ollama, model.to_string())
        .with_tools(tools);

    // Add system prompt
    coordinator.add_message(ChatMessage::system(system_prompt.to_string()));

    // Run the agent loop
    let response = coordinator.chat(user_message.to_string()).await?;

    // Stream content to callback
    for line in response.lines() {
        callback(line)?;
    }

    Ok(response)
}
```

### 4. Update OllamaProvider in src/provider.rs

Replace current HTTP-based implementation with ollama-rs Coordinator:

```rust
impl ModelProvider for OllamaProvider {
    fn invoke(
        &self,
        message: &str,
        model: &str,
        callback: &mut dyn FnMut(&str) -> Result<()>,
    ) -> Result<String> {
        // Use tokio runtime for async ollama-rs
        let rt = tokio::runtime::Runtime::new()?;
        rt.block_on(crate::agent::run_agent(
            &self.endpoint,
            model,
            "", // system prompt from caller
            message,
            callback,
        ))
    }
}
```

## Model Recommendations

For reliable tool calling with ollama:
- `qwen2.5:7b` or larger - good tool calling support
- `hhao/qwen2.5-coder-tools` - community variant of qwen2.5-coder with tool support
- `qwen3-coder` (when available) - native agentic workflow support
- `llama3.1:8b` or larger - solid tool calling

Avoid `qwen2.5-coder` base models for tool calling (optimized for code gen, not function calling).

## Acceptance Criteria

- [ ] `ollama-rs` added to Cargo.toml with function-calling feature
- [ ] `src/tools.rs` created with read_file, write_file, run_command, list_files tools
- [ ] `src/agent.rs` created with Coordinator-based agent loop
- [ ] `OllamaProvider::invoke` uses ollama-rs Coordinator instead of raw HTTP
- [ ] Tool executions are logged/visible during spec execution
- [ ] `just check` passes
- [ ] Manual test: `just chant work <spec-id>` with ollama executes tools and modifies files

## Dependencies

- `ollama-rs = { version = "0.3", features = ["function-calling"] }`
- `tokio = { version = "1", features = ["rt-multi-thread"] }` (if not already present)
- `glob = "0.3"` (for list_files)

## Notes

- This changes ollama from "chat completion" mode to "agent" mode
- Claude CLI provider remains unchanged (already has agent runtime)
- OpenAI provider could also use this pattern in future
